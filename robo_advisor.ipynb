{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "A simple robo-advisor that incorporates sentiment analysis from news headlines with stock positions to make recommendations and execute trades. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from warnings import filterwarnings\n",
    "import alpaca_trade_api as tradeapi\n",
    "from alpaca_trade_api.rest import APIError\n",
    "from transformers import pipeline, BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "ALPACA_API_KEY = os.getenv(\"ALPACA_API_KEY\")\n",
    "ALPACA_SECRET_KEY = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "ALPACA_URL = os.getenv(\"ALPACA_URL\")\n",
    "\n",
    "alpaca = tradeapi.REST(ALPACA_API_KEY, ALPACA_SECRET_KEY, base_url=ALPACA_URL, api_version='v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 50 Stocks from S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_sp500_stocks = [\n",
    "    'AAPL',  # Apple Inc.\n",
    "    'MSFT',  # Microsoft Corporation\n",
    "    'AMZN',  # Amazon.com Inc.\n",
    "    'NVDA',  # NVIDIA Corporation\n",
    "    'GOOGL', # Alphabet Inc. (Class A)\n",
    "    'GOOG',  # Alphabet Inc. (Class C)\n",
    "    'TSLA',  # Tesla Inc.\n",
    "    'META',  # Meta Platforms Inc.\n",
    "    'BRK.B', # Berkshire Hathaway Inc. (Class B)\n",
    "    'UNH',   # UnitedHealth Group Incorporated\n",
    "    'JNJ',   # Johnson & Johnson\n",
    "    'XOM',   # Exxon Mobil Corporation\n",
    "    'V',     # Visa Inc.\n",
    "    'PG',    # Procter & Gamble Co.\n",
    "    'JPM',   # JPMorgan Chase & Co.\n",
    "    'LLY',   # Eli Lilly and Company\n",
    "    'MA',    # Mastercard Incorporated\n",
    "    'HD',    # The Home Depot Inc.\n",
    "    'CVX',   # Chevron Corporation\n",
    "    'MRK',   # Merck & Co. Inc.\n",
    "    'PEP',   # PepsiCo Inc.\n",
    "    'ABBV',  # AbbVie Inc.\n",
    "    'KO',    # The Coca-Cola Company\n",
    "    'PFE',   # Pfizer Inc.\n",
    "    'AVGO',  # Broadcom Inc.\n",
    "    'COST',  # Costco Wholesale Corporation\n",
    "    'MCD',   # McDonald's Corporation\n",
    "    'TMO',   # Thermo Fisher Scientific Inc.\n",
    "    'WMT',   # Walmart Inc.\n",
    "    'DHR',   # Danaher Corporation\n",
    "    'NKE',   # NIKE Inc.\n",
    "    'DIS',   # The Walt Disney Company\n",
    "    'ADBE',  # Adobe Inc.\n",
    "    'NFLX',  # Netflix Inc.\n",
    "    'VZ',    # Verizon Communications Inc.\n",
    "    'CSCO',  # Cisco Systems Inc.\n",
    "    'ABT',   # Abbott Laboratories\n",
    "    'ACN',   # Accenture plc\n",
    "    'NEE',   # NextEra Energy Inc.\n",
    "    'LIN',   # Linde plc\n",
    "    'TXN',   # Texas Instruments Incorporated\n",
    "    'MDT',   # Medtronic plc\n",
    "    'PM',    # Philip Morris International Inc.\n",
    "    'WFC',   # Wells Fargo & Company\n",
    "    'HON',   # Honeywell International Inc.\n",
    "    'QCOM',  # QUALCOMM Incorporated\n",
    "    'BMY',   # Bristol-Myers Squibb Company\n",
    "    'LOW',   # Lowe's Companies Inc.\n",
    "    'UNP',   # Union Pacific Corporation\n",
    "    'RTX'    # Raytheon Technologies Corporation\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_stock_data(symbols, start_date, end_date):\n",
    "    timeframe = tradeapi.TimeFrame.Day\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        bars = alpaca.get_bars(\n",
    "            symbol,\n",
    "            timeframe=timeframe,\n",
    "            start=start_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            end=end_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            adjustment='raw',\n",
    "            feed='iex'\n",
    "        )\n",
    "        \n",
    "        data = []\n",
    "        for bar in bars:\n",
    "            data.append({\n",
    "                'symbol': symbol, \n",
    "                'time': bar.t,\n",
    "                'open': bar.o,\n",
    "                'high': bar.h,\n",
    "                'low': bar.l,\n",
    "                'close': bar.c,\n",
    "                'volume': bar.v\n",
    "            })\n",
    "        \n",
    "        all_data.extend(data)\n",
    "    \n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(symbols, days_prior=1):\n",
    "    today = datetime.utcnow()\n",
    "    yesterday = today - timedelta(days=days_prior)  \n",
    "    url = \"https://data.alpaca.markets/v1beta1/news\"\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"APCA-API-KEY-ID\": ALPACA_API_KEY,\n",
    "        \"APCA-API-SECRET-KEY\": ALPACA_SECRET_KEY\n",
    "    }\n",
    "    news = []\n",
    "    page_token = None\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"symbols\": \",\".join(symbols),\n",
    "            \"start\": start_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            \"end\": today.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            \"limit\": 50,  \n",
    "            \"page_token\": page_token  \n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            \n",
    "            news.extend(result.get('news', []))\n",
    "            \n",
    "            page_token = result.get('next_page_token')\n",
    "            \n",
    "            if not page_token:  \n",
    "                break\n",
    "            \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 429:\n",
    "                print(\"Rate limit reached. Sleeping for a few seconds...\")\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                print(f\"HTTP error occurred: {e}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "        \n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(text, max_length=500):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=False)['input_ids'][0]\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunk = tokens[i:i + max_length]\n",
    "        if len(chunk) > max_length:\n",
    "            chunk = chunk[:max_length]\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    chunks = split_into_chunks(text)\n",
    "    sentiments = []\n",
    "    for chunk in chunks:\n",
    "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        sentiments.append(nlp(chunk_text)[0])\n",
    "    \n",
    "    avg_sentiment_score = sum(s['score'] for s in sentiments) / len(sentiments)\n",
    "    positive_scores = sum(s['score'] for s in sentiments if s['label'] == 'positive')\n",
    "    negative_scores = sum(s['score'] for s in sentiments if s['label'] == 'negative')\n",
    "    sentiment_label = 'positive' if positive_scores >= negative_scores else 'negative'\n",
    "    \n",
    "    return sentiment_label, avg_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_last_update_date(file_path=\"stock_data.csv\"):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "        df = df.dropna(subset=['date'])\n",
    "\n",
    "        most_recent_date = df['date'].max()\n",
    "\n",
    "        return most_recent_date\n",
    "    except (FileNotFoundError, IndexError, KeyError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_last_update_date(date, file_path=\"stock_data.csv\"):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.loc[df['date'].idxmax(), 'date'] = date\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_news(symbols, start_date, end_date):\n",
    "    url = \"https://data.alpaca.markets/v1beta1/news\"\n",
    "    headers = {\n",
    "        \"APCA-API-KEY-ID\": ALPACA_API_KEY,\n",
    "        \"APCA-API-SECRET-KEY\": ALPACA_SECRET_KEY\n",
    "    }\n",
    "    news = []\n",
    "    page_token = None\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"symbols\": \",\".join(symbols),\n",
    "            \"start\": start_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            \"end\": end_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            \"limit\": 50,\n",
    "            \"page_token\": page_token\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            \n",
    "            news.extend(result.get('news', []))\n",
    "            \n",
    "            page_token = result.get('next_page_token')\n",
    "            if not page_token:\n",
    "                break\n",
    "            \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 429:\n",
    "                print(\"Rate limit reached. Sleeping for a few seconds...\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(f\"HTTP error occurred: {e}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "        \n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_stock_data(symbols, file_path=\"stock_data.csv\"):\n",
    "    last_update = load_last_update_date(file_path)\n",
    "    if last_update is None:\n",
    "        last_update = datetime.utcnow() - timedelta(days=30)  \n",
    "    else:\n",
    "        last_update = pd.to_datetime(last_update)  \n",
    "    \n",
    "    today = datetime.utcnow()\n",
    "\n",
    "    if last_update.date() == today.date():\n",
    "        print(\"Data is already up-to-date. No new data to fetch.\")\n",
    "        return\n",
    "    \n",
    "    new_news = get_new_news(symbols, last_update, today)\n",
    "    \n",
    "    new_news_df = pd.DataFrame(new_news)\n",
    "\n",
    "    if not new_news_df.empty:\n",
    "        new_news_df['created_at'] = pd.to_datetime(new_news_df['created_at'])\n",
    "        new_news_df['date'] = new_news_df['created_at'].dt.date\n",
    "        new_news_df = new_news_df.explode('symbols')\n",
    "        new_news_df['sentiment'], new_news_df['sentiment_score'] = zip(*new_news_df['headline'].apply(analyze_sentiment))\n",
    "        new_news_df = new_news_df[['date', 'symbols', 'sentiment_score']]\n",
    "        daily_sentiment = new_news_df.groupby(['date', 'symbols'])['sentiment_score'].mean().reset_index()\n",
    "        daily_sentiment.columns = ['date', 'symbol', 'average_sentiment_score']\n",
    "    else:\n",
    "        daily_sentiment = pd.DataFrame()\n",
    "\n",
    "    historical_data = get_historical_stock_data(symbols, last_update, today)\n",
    "\n",
    "    historical_data['date'] = pd.to_datetime(historical_data['time']).dt.date\n",
    "\n",
    "    stock_data_merged = pd.merge(historical_data, daily_sentiment, how='left', on=['date', 'symbol'])\n",
    "\n",
    "    try:\n",
    "        existing_data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        existing_data = pd.DataFrame()\n",
    "\n",
    "    combined_data = pd.concat([existing_data, stock_data_merged], ignore_index=True)\n",
    "    \n",
    "    combined_data.to_csv(file_path, index=False)\n",
    "    \n",
    "    save_last_update_date(today, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    \"\"\"\n",
    "    Add moving averages, EMA, and time trend to the dataframe.\n",
    "    \"\"\"\n",
    "    df = df.copy()  \n",
    "    df.loc[:, '5_day_sma'] = df['close'].rolling(window=5).mean()\n",
    "    df.loc[:, '5_day_ema'] = df['close'].ewm(span=5, adjust=False).mean()\n",
    "    df.loc[:, 'time_trend'] = np.arange(len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_path='stock_data.csv'):\n",
    "    \"\"\"\n",
    "    Prepare the data by handling missing values, adding features, and splitting into features and target.\n",
    "    \"\"\"\n",
    "    stock_data = pd.read_csv(file_path)\n",
    "\n",
    "    stock_data['time'] = pd.to_datetime(stock_data['time'])\n",
    "\n",
    "    stock_data = add_features(stock_data)\n",
    "\n",
    "    feature_columns = ['close', 'log_sentiment_score', '5_day_sma', '5_day_ema', 'time_trend']\n",
    "    X = stock_data[feature_columns]\n",
    "    y = stock_data['close'].shift(-1) \n",
    "\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "\n",
    "    X = X[:-1]\n",
    "    y = y.dropna()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    return X, y, imputer, scaler, feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y):\n",
    "    \"\"\"\n",
    "    Train a LinearRegression model on the provided data.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_decide(symbols, model, imputer, scaler, feature_columns, file_path='stock_data.csv'):\n",
    "    \"\"\"\n",
    "    Predict the next day's stock price for each symbol and decide whether to buy, sell, or hold.\n",
    "    \"\"\"\n",
    "    stock_data = pd.read_csv(file_path)\n",
    "\n",
    "    decisions = []\n",
    "\n",
    "    for symbol in symbols:\n",
    "        symbol_data = stock_data[stock_data['symbol'] == symbol]\n",
    "\n",
    "        symbol_data = add_features(symbol_data)\n",
    "\n",
    "        latest_data = symbol_data.iloc[-1]\n",
    "\n",
    "        X_latest = latest_data[feature_columns].values.reshape(1, -1)\n",
    "\n",
    "        X_latest = imputer.transform(X_latest)\n",
    "\n",
    "        X_latest = scaler.transform(X_latest)\n",
    "\n",
    "        predicted_price = model.predict(X_latest)[0]\n",
    "\n",
    "        current_price = latest_data['close']\n",
    "\n",
    "        if predicted_price > current_price * 1.02:\n",
    "            decision = 'buy'\n",
    "        elif predicted_price < current_price * 0.98:\n",
    "            decision = 'sell'\n",
    "        else:\n",
    "            decision = 'hold'\n",
    "\n",
    "        decisions.append((symbol, decision, current_price, predicted_price))\n",
    "\n",
    "    return decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_trade(symbol, decision, hedge_sell_ratio=0.01, max_investment_ratio=0.01):\n",
    "    account = alpaca.get_account()\n",
    "    cash = float(account.cash)\n",
    "    \n",
    "    position = alpaca.get_position(symbol) if symbol in [p.symbol for p in alpaca.list_positions()] else None\n",
    "    current_qty = float(position.qty) if position else 0\n",
    "\n",
    "    current_price = None\n",
    "    for dec_symbol, dec, curr_price, _ in decisions:\n",
    "        if dec_symbol == symbol:\n",
    "            current_price = curr_price\n",
    "            break\n",
    "\n",
    "    if current_price is None:\n",
    "        print(f\"Current price for {symbol} not found in decisions.\")\n",
    "        return\n",
    "\n",
    "    if decision == 'buy':\n",
    "        qty_to_buy = (cash * max_investment_ratio) / current_price\n",
    "        \n",
    "        qty_to_buy = int(qty_to_buy)\n",
    "\n",
    "        if qty_to_buy > 0:\n",
    "            try:\n",
    "                alpaca.submit_order(\n",
    "                    symbol=symbol,\n",
    "                    qty=qty_to_buy,\n",
    "                    side='buy',\n",
    "                    type='market',\n",
    "                    time_in_force='day'\n",
    "                )\n",
    "                print(f\"Successfully placed a buy order for {qty_to_buy} shares of {symbol}.\")\n",
    "            except APIError as e:\n",
    "                print(f\"Failed to place a buy order: {e}\")\n",
    "        else:\n",
    "            print(f\"Insufficient funds to buy any shares of {symbol}.\")\n",
    "            \n",
    "    elif decision == 'sell' and current_qty > 0:\n",
    "        qty_to_sell = current_qty * hedge_sell_ratio\n",
    "        \n",
    "        qty_to_sell = int(qty_to_sell)\n",
    "\n",
    "        if qty_to_sell > 0:\n",
    "            try:\n",
    "                alpaca.submit_order(\n",
    "                    symbol=symbol,\n",
    "                    qty=qty_to_sell,\n",
    "                    side='sell',\n",
    "                    type='market',\n",
    "                    time_in_force='day'\n",
    "                )\n",
    "                print(f\"Successfully placed a sell order for {qty_to_sell} shares of {symbol}.\")\n",
    "            except APIError as e:\n",
    "                print(f\"Failed to place a sell order: {e}\")\n",
    "        else:\n",
    "            print(f\"Not enough shares to sell for {symbol}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime.utcnow()\n",
    "start_date = end_date - timedelta(days=30)\n",
    "historical_data = get_historical_stock_data(top_50_sp500_stocks, start_date, end_date)\n",
    "historical_data['date'] = pd.to_datetime(historical_data['time']).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = get_news(top_50_sp500_stocks, days_prior=30)\n",
    "news_df = pd.DataFrame(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences\n",
    "- **Finbert Model** for sentiment analysis based inference \n",
    "- **Linear Regression** for stock price inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"yiyanghkust/finbert-tone\" \n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['sentiment'], news_df['sentiment_score'] = zip(*news_df['headline'].apply(analyze_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['created_at'] = pd.to_datetime(news_df['created_at'])\n",
    "\n",
    "news_df['date'] = news_df['created_at'].dt.date\n",
    "\n",
    "news_copy = news_df.explode('symbols')\n",
    "\n",
    "daily_sentiment = news_copy.groupby(['date', 'symbols'])['sentiment_score'].mean().reset_index()\n",
    "\n",
    "daily_sentiment.columns = ['date', 'symbol', 'average_sentiment_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sentiment['log_sentiment_score'] = daily_sentiment['average_sentiment_score'].apply(lambda x: math.log(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_merged = pd.merge(historical_data, daily_sentiment, how='left', on=['date', 'symbol'])\n",
    "stock_data_merged.to_csv('stock_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_stock_data(top_50_sp500_stocks)\n",
    "stock_data = pd.read_csv(\"stock_data.csv\")\n",
    "data = pd.read_csv('stock_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "data[['open', 'high', 'low', 'close', 'volume', 'average_sentiment_score']] = scaler.fit_transform(\n",
    "    data[['open', 'high', 'low', 'close', 'volume', 'average_sentiment_score']]\n",
    ")\n",
    "\n",
    "data['next_day_close'] = data.groupby('symbol')['close'].shift(-1)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "X = data[['open', 'high', 'low', 'close', 'volume', 'average_sentiment_score']]\n",
    "y = data['next_day_close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = pd.read_csv('stock_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, imputer, scaler, feature_columns = prepare_data()\n",
    "\n",
    "model = train_model(X, y)\n",
    "\n",
    "decisions = predict_and_decide(top_50_sp500_stocks, model, imputer, scaler, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    X, y, imputer, scaler, feature_columns = prepare_data()\n",
    "\n",
    "    model = train_model(X, y)\n",
    "\n",
    "    symbols = top_50_sp500_stocks\n",
    "\n",
    "    decisions = predict_and_decide(symbols, model, imputer, scaler, feature_columns)\n",
    "\n",
    "    for symbol, decision, current_price, predicted_price in decisions:\n",
    "        execute_trade(symbol, decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully placed a buy order for 2 shares of AAPL.\n",
      "Failed to place a buy order: insufficient qty available for order (requested: 3, available: 1.223989371)\n",
      "Successfully placed a buy order for 4 shares of NVDA.\n",
      "Successfully placed a buy order for 3 shares of GOOGL.\n",
      "Successfully placed a buy order for 3 shares of GOOG.\n",
      "Successfully placed a buy order for 2 shares of TSLA.\n",
      "Successfully placed a buy order for 3 shares of JNJ.\n",
      "Successfully placed a buy order for 4 shares of XOM.\n",
      "Successfully placed a buy order for 2 shares of V.\n",
      "Successfully placed a buy order for 3 shares of PG.\n",
      "Successfully placed a buy order for 2 shares of JPM.\n",
      "Successfully placed a buy order for 3 shares of CVX.\n",
      "Successfully placed a buy order for 4 shares of MRK.\n",
      "Successfully placed a buy order for 3 shares of PEP.\n",
      "Successfully placed a buy order for 2 shares of ABBV.\n",
      "Successfully placed a buy order for 7 shares of KO.\n",
      "Successfully placed a buy order for 19 shares of PFE.\n",
      "Successfully placed a buy order for 3 shares of AVGO.\n",
      "Successfully placed a buy order for 1 shares of MCD.\n",
      "Successfully placed a buy order for 7 shares of WMT.\n",
      "Successfully placed a buy order for 2 shares of DHR.\n",
      "Successfully placed a buy order for 6 shares of NKE.\n",
      "Successfully placed a buy order for 6 shares of DIS.\n",
      "Successfully placed a buy order for 13 shares of VZ.\n",
      "Successfully placed a buy order for 11 shares of CSCO.\n",
      "Successfully placed a buy order for 4 shares of ABT.\n",
      "Successfully placed a buy order for 6 shares of NEE.\n",
      "Successfully placed a buy order for 2 shares of TXN.\n",
      "Successfully placed a buy order for 6 shares of MDT.\n",
      "Successfully placed a buy order for 4 shares of PM.\n",
      "Successfully placed a buy order for 9 shares of WFC.\n",
      "Successfully placed a buy order for 2 shares of HON.\n",
      "Successfully placed a buy order for 3 shares of QCOM.\n",
      "Successfully placed a buy order for 11 shares of BMY.\n",
      "Successfully placed a buy order for 2 shares of LOW.\n",
      "Successfully placed a buy order for 2 shares of UNP.\n",
      "Successfully placed a buy order for 4 shares of RTX.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
