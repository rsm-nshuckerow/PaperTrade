{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import alpaca_trade_api as tradeapi\n",
    "import pandas as pd\n",
    "from transformers import pipeline, BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "ALPACA_API_KEY = os.getenv(\"ALPACA_API_KEY\")\n",
    "ALPACA_SECRET_KEY = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "ALPACA_URL = os.getenv(\"ALPACA_URL\")\n",
    "\n",
    "\n",
    "alpaca = tradeapi.REST(ALPACA_API_KEY, ALPACA_SECRET_KEY, base_url=ALPACA_URL, api_version='v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"yiyanghkust/finbert-tone\"  # Example of a financial sentiment model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into 512-token chunks based on tokenization\n",
    "def split_into_chunks(text, max_length=500):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=False)['input_ids'][0]\n",
    "    # Ensure each chunk is no more than 512 tokens\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunk = tokens[i:i + max_length]\n",
    "        # Make sure the chunk is exactly 512 tokens or less\n",
    "        if len(chunk) > max_length:\n",
    "            chunk = chunk[:max_length]\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze sentiment for long texts\n",
    "def analyze_sentiment(text):\n",
    "    chunks = split_into_chunks(text)\n",
    "    sentiments = []\n",
    "    for chunk in chunks:\n",
    "        # Convert tokens back to text before sentiment analysis\n",
    "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        sentiments.append(nlp(chunk_text)[0])\n",
    "    \n",
    "    # Aggregate sentiment scores (e.g., by averaging)\n",
    "    avg_sentiment_score = sum(s['score'] for s in sentiments) / len(sentiments)\n",
    "    # Determine overall sentiment by majority vote or averaging\n",
    "    positive_scores = sum(s['score'] for s in sentiments if s['label'] == 'positive')\n",
    "    negative_scores = sum(s['score'] for s in sentiments if s['label'] == 'negative')\n",
    "    sentiment_label = 'positive' if positive_scores >= negative_scores else 'negative'\n",
    "    \n",
    "    return sentiment_label, avg_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_stock_data(symbols, start_date, end_date):\n",
    "    # Use the correct TimeFrame object for daily data\n",
    "    timeframe = tradeapi.TimeFrame.Day\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        bars = alpaca.get_bars(\n",
    "            symbol,\n",
    "            timeframe=timeframe,\n",
    "            start=start_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            end=end_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            adjustment='raw',\n",
    "            feed='iex'\n",
    "        )\n",
    "        \n",
    "        data = []\n",
    "        for bar in bars:\n",
    "            data.append({\n",
    "                'symbol': symbol,  # Add the symbol to the data\n",
    "                'time': bar.t,\n",
    "                'open': bar.o,\n",
    "                'high': bar.h,\n",
    "                'low': bar.l,\n",
    "                'close': bar.c,\n",
    "                'volume': bar.v\n",
    "            })\n",
    "        \n",
    "        all_data.extend(data)\n",
    "    \n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the most recent date from the CSV file\n",
    "def load_last_update_date(file_path=\"stock_data.csv\"):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure that the 'date' column is converted to datetime, coercing errors\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "        # Drop rows where 'date' couldn't be converted to datetime (i.e., NaT)\n",
    "        df = df.dropna(subset=['date'])\n",
    "\n",
    "        # Find the most recent date\n",
    "        most_recent_date = df['date'].max()\n",
    "\n",
    "        return most_recent_date\n",
    "    except (FileNotFoundError, IndexError, KeyError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the current date as the last update date\n",
    "def save_last_update_date(date, file_path=\"stock_data.csv\"):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.loc[df['date'].idxmax(), 'date'] = date\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get news from the Alpaca API\n",
    "def get_new_news(symbols, start_date, end_date):\n",
    "    url = \"https://data.alpaca.markets/v1beta1/news\"\n",
    "    headers = {\n",
    "        \"APCA-API-KEY-ID\": ALPACA_API_KEY,\n",
    "        \"APCA-API-SECRET-KEY\": ALPACA_SECRET_KEY\n",
    "    }\n",
    "    news = []\n",
    "    page_token = None\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"symbols\": \",\".join(symbols),\n",
    "            \"start\": start_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            \"end\": end_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            \"limit\": 50,\n",
    "            \"page_token\": page_token\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            \n",
    "            news.extend(result.get('news', []))\n",
    "            \n",
    "            page_token = result.get('next_page_token')\n",
    "            if not page_token:\n",
    "                break\n",
    "            \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 429:\n",
    "                print(\"Rate limit reached. Sleeping for 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(f\"HTTP error occurred: {e}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "        \n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_stock_data(symbols, file_path=\"stock_data.csv\"):\n",
    "    # Load the last update date\n",
    "    last_update = load_last_update_date(file_path)\n",
    "    if last_update is None:\n",
    "        last_update = datetime.utcnow() - timedelta(days=30)  # Default to last 30 days\n",
    "    else:\n",
    "        last_update = pd.to_datetime(last_update)  # Convert to datetime\n",
    "    \n",
    "    # Get the current date\n",
    "    today = datetime.utcnow()\n",
    "\n",
    "    # Stop execution if last_update and today are the same\n",
    "    if last_update.date() == today.date():\n",
    "        print(\"Data is already up-to-date. No new data to fetch.\")\n",
    "        return\n",
    "    \n",
    "    # Retrieve news from Alpaca\n",
    "    new_news = get_new_news(symbols, last_update, today)\n",
    "    \n",
    "    # Convert new news data to DataFrame\n",
    "    new_news_df = pd.DataFrame(new_news)\n",
    "\n",
    "    # Keep only the necessary columns and prepare the data\n",
    "    if not new_news_df.empty:\n",
    "        new_news_df['created_at'] = pd.to_datetime(new_news_df['created_at'])\n",
    "        new_news_df['date'] = new_news_df['created_at'].dt.date\n",
    "        new_news_df = new_news_df.explode('symbols')\n",
    "        new_news_df['sentiment'], new_news_df['sentiment_score'] = zip(*new_news_df['headline'].apply(analyze_sentiment))\n",
    "        new_news_df = new_news_df[['date', 'symbols', 'sentiment_score']]\n",
    "        daily_sentiment = new_news_df.groupby(['date', 'symbols'])['sentiment_score'].mean().reset_index()\n",
    "        daily_sentiment.columns = ['date', 'symbol', 'average_sentiment_score']\n",
    "    else:\n",
    "        daily_sentiment = pd.DataFrame()\n",
    "\n",
    "    # Fetch historical stock data\n",
    "    historical_data = get_historical_stock_data(symbols, last_update, today)\n",
    "\n",
    "    historical_data['date'] = pd.to_datetime(historical_data['time']).dt.date\n",
    "\n",
    "    # Merge historical stock data with sentiment data\n",
    "    stock_data_merged = pd.merge(historical_data, daily_sentiment, how='left', on=['date', 'symbol'])\n",
    "\n",
    "    # Load existing stock data\n",
    "    try:\n",
    "        existing_data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        existing_data = pd.DataFrame()\n",
    "\n",
    "    # Combine the new merged data with the existing data\n",
    "    combined_data = pd.concat([existing_data, stock_data_merged], ignore_index=True)\n",
    "    \n",
    "    # Save the updated DataFrame\n",
    "    combined_data.to_csv(file_path, index=False)\n",
    "    \n",
    "    # Save the most recent update date\n",
    "    save_last_update_date(today, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_sp500_stocks = [\n",
    "    'AAPL',  # Apple Inc.\n",
    "    'MSFT',  # Microsoft Corporation\n",
    "    'AMZN',  # Amazon.com Inc.\n",
    "    'NVDA',  # NVIDIA Corporation\n",
    "    'GOOGL', # Alphabet Inc. (Class A)\n",
    "    'GOOG',  # Alphabet Inc. (Class C)\n",
    "    'TSLA',  # Tesla Inc.\n",
    "    'META',  # Meta Platforms Inc.\n",
    "    'BRK.B', # Berkshire Hathaway Inc. (Class B)\n",
    "    'UNH',   # UnitedHealth Group Incorporated\n",
    "    'JNJ',   # Johnson & Johnson\n",
    "    'XOM',   # Exxon Mobil Corporation\n",
    "    'V',     # Visa Inc.\n",
    "    'PG',    # Procter & Gamble Co.\n",
    "    'JPM',   # JPMorgan Chase & Co.\n",
    "    'LLY',   # Eli Lilly and Company\n",
    "    'MA',    # Mastercard Incorporated\n",
    "    'HD',    # The Home Depot Inc.\n",
    "    'CVX',   # Chevron Corporation\n",
    "    'MRK',   # Merck & Co. Inc.\n",
    "    'PEP',   # PepsiCo Inc.\n",
    "    'ABBV',  # AbbVie Inc.\n",
    "    'KO',    # The Coca-Cola Company\n",
    "    'PFE',   # Pfizer Inc.\n",
    "    'AVGO',  # Broadcom Inc.\n",
    "    'COST',  # Costco Wholesale Corporation\n",
    "    'MCD',   # McDonald's Corporation\n",
    "    'TMO',   # Thermo Fisher Scientific Inc.\n",
    "    'WMT',   # Walmart Inc.\n",
    "    'DHR',   # Danaher Corporation\n",
    "    'NKE',   # NIKE Inc.\n",
    "    'DIS',   # The Walt Disney Company\n",
    "    'ADBE',  # Adobe Inc.\n",
    "    'NFLX',  # Netflix Inc.\n",
    "    'VZ',    # Verizon Communications Inc.\n",
    "    'CSCO',  # Cisco Systems Inc.\n",
    "    'ABT',   # Abbott Laboratories\n",
    "    'ACN',   # Accenture plc\n",
    "    'NEE',   # NextEra Energy Inc.\n",
    "    'LIN',   # Linde plc\n",
    "    'TXN',   # Texas Instruments Incorporated\n",
    "    'MDT',   # Medtronic plc\n",
    "    'PM',    # Philip Morris International Inc.\n",
    "    'WFC',   # Wells Fargo & Company\n",
    "    'HON',   # Honeywell International Inc.\n",
    "    'QCOM',  # QUALCOMM Incorporated\n",
    "    'BMY',   # Bristol-Myers Squibb Company\n",
    "    'LOW',   # Lowe's Companies Inc.\n",
    "    'UNP',   # Union Pacific Corporation\n",
    "    'RTX'    # Raytheon Technologies Corporation\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP error occurred: 403 Client Error: Forbidden for url: https://data.alpaca.markets/v1beta1/news?symbols=AAPL%2CMSFT%2CAMZN%2CNVDA%2CGOOGL%2CGOOG%2CTSLA%2CMETA%2CBRK.B%2CUNH%2CJNJ%2CXOM%2CV%2CPG%2CJPM%2CLLY%2CMA%2CHD%2CCVX%2CMRK%2CPEP%2CABBV%2CKO%2CPFE%2CAVGO%2CCOST%2CMCD%2CTMO%2CWMT%2CDHR%2CNKE%2CDIS%2CADBE%2CNFLX%2CVZ%2CCSCO%2CABT%2CACN%2CNEE%2CLIN%2CTXN%2CMDT%2CPM%2CWFC%2CHON%2CQCOM%2CBMY%2CLOW%2CUNP%2CRTX&start=2024-08-27T01%3A14%3A06Z&end=2024-08-31T19%3A56%3A34Z&limit=50\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/79/jg9qrvxj5v1_ft3s9qfkpbl40000gn/T/ipykernel_28649/2921376217.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mupdate_stock_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_50_sp500_stocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/79/jg9qrvxj5v1_ft3s9qfkpbl40000gn/T/ipykernel_28649/686573454.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(symbols, file_path)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mhistorical_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorical_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Merge historical stock data with sentiment data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mstock_data_merged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorical_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaily_sentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'symbol'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Load existing stock data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1906\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1907\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1910\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "update_stock_data(top_50_sp500_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol                       time     open    high     low    close  volume  \\\n",
      "0   AAPL  2024-07-25 00:00:00-04:00  218.880  220.81  214.64  217.420  642703   \n",
      "1   AAPL  2024-07-26 00:00:00-04:00  218.940  219.48  216.04  218.030  661067   \n",
      "2   AAPL  2024-07-29 00:00:00-04:00  217.375  219.28  215.79  218.185  381966   \n",
      "3   AAPL  2024-07-30 00:00:00-04:00  219.300  220.27  216.12  218.680  559407   \n",
      "4   AAPL  2024-07-31 00:00:00-04:00  221.520  223.81  220.91  222.180  549326   \n",
      "\n",
      "                         date  average_sentiment_score  log_sentiment_score  \n",
      "0  2024-07-25 00:00:00.000000                 0.947577            -0.053847  \n",
      "1  2024-07-26 00:00:00.000000                 0.972036            -0.028362  \n",
      "2  2024-07-29 00:00:00.000000                 0.955637            -0.045377  \n",
      "3  2024-07-30 00:00:00.000000                 0.964756            -0.035880  \n",
      "4  2024-07-31 00:00:00.000000                 0.989995            -0.010056  \n"
     ]
    }
   ],
   "source": [
    "# read stock data and display head\n",
    "\n",
    "stock_data = pd.read_csv(\"stock_data.csv\")\n",
    "print(stock_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
