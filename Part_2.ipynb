{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import alpaca_trade_api as tradeapi\n",
    "import pandas as pd\n",
    "from transformers import pipeline, BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca API credentials\n",
    "ALPACA_API_KEY = \"PKDFQIPQCXE38TAG1WKG\"\n",
    "ALPACA_SECRET_KEY = \"LRaWwamsMm0WDa58x8S0z8wje9gRcYhcgtMf7C55\"\n",
    "ALPACA_URL = 'https://paper-api.alpaca.markets'\n",
    "\n",
    "# Initialize Alpaca API\n",
    "alpaca = tradeapi.REST(ALPACA_API_KEY, ALPACA_SECRET_KEY, base_url=ALPACA_URL, api_version='v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"yiyanghkust/finbert-tone\"  # Example of a financial sentiment model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into 512-token chunks based on tokenization\n",
    "def split_into_chunks(text, max_length=500):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=False)['input_ids'][0]\n",
    "    # Ensure each chunk is no more than 512 tokens\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunk = tokens[i:i + max_length]\n",
    "        # Make sure the chunk is exactly 512 tokens or less\n",
    "        if len(chunk) > max_length:\n",
    "            chunk = chunk[:max_length]\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze sentiment for long texts\n",
    "def analyze_sentiment(text):\n",
    "    chunks = split_into_chunks(text)\n",
    "    sentiments = []\n",
    "    for chunk in chunks:\n",
    "        # Convert tokens back to text before sentiment analysis\n",
    "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        sentiments.append(nlp(chunk_text)[0])\n",
    "    \n",
    "    # Aggregate sentiment scores (e.g., by averaging)\n",
    "    avg_sentiment_score = sum(s['score'] for s in sentiments) / len(sentiments)\n",
    "    # Determine overall sentiment by majority vote or averaging\n",
    "    positive_scores = sum(s['score'] for s in sentiments if s['label'] == 'positive')\n",
    "    negative_scores = sum(s['score'] for s in sentiments if s['label'] == 'negative')\n",
    "    sentiment_label = 'positive' if positive_scores >= negative_scores else 'negative'\n",
    "    \n",
    "    return sentiment_label, avg_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_stock_data(symbols, start_date, end_date):\n",
    "    # Use the correct TimeFrame object for daily data\n",
    "    timeframe = tradeapi.TimeFrame.Day\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        bars = alpaca.get_bars(\n",
    "            symbol,\n",
    "            timeframe=timeframe,\n",
    "            start=start_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            end=end_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            adjustment='raw',\n",
    "            feed='iex'\n",
    "        )\n",
    "        \n",
    "        data = []\n",
    "        for bar in bars:\n",
    "            data.append({\n",
    "                'symbol': symbol,  # Add the symbol to the data\n",
    "                'time': bar.t,\n",
    "                'open': bar.o,\n",
    "                'high': bar.h,\n",
    "                'low': bar.l,\n",
    "                'close': bar.c,\n",
    "                'volume': bar.v\n",
    "            })\n",
    "        \n",
    "        all_data.extend(data)\n",
    "    \n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the most recent date from the CSV file\n",
    "def load_last_update_date(file_path=\"stock_data.csv\"):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure that the 'date' column is converted to datetime, coercing errors\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "        # Drop rows where 'date' couldn't be converted to datetime (i.e., NaT)\n",
    "        df = df.dropna(subset=['date'])\n",
    "\n",
    "        # Find the most recent date\n",
    "        most_recent_date = df['date'].max()\n",
    "\n",
    "        return most_recent_date\n",
    "    except (FileNotFoundError, IndexError, KeyError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the current date as the last update date\n",
    "def save_last_update_date(date, file_path=\"stock_data.csv\"):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.loc[df['date'].idxmax(), 'date'] = date\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get news from the Alpaca API\n",
    "def get_new_news(symbols, start_date, end_date):\n",
    "    url = \"https://data.alpaca.markets/v1beta1/news\"\n",
    "    headers = {\n",
    "        \"APCA-API-KEY-ID\": ALPACA_API_KEY,\n",
    "        \"APCA-API-SECRET-KEY\": ALPACA_SECRET_KEY\n",
    "    }\n",
    "    news = []\n",
    "    page_token = None\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"symbols\": \",\".join(symbols),\n",
    "            \"start\": start_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            \"end\": end_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            \"limit\": 50,\n",
    "            \"page_token\": page_token\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            \n",
    "            news.extend(result.get('news', []))\n",
    "            \n",
    "            page_token = result.get('next_page_token')\n",
    "            if not page_token:\n",
    "                break\n",
    "            \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 429:\n",
    "                print(\"Rate limit reached. Sleeping for 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(f\"HTTP error occurred: {e}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "        \n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_stock_data(symbols, file_path=\"stock_data.csv\"):\n",
    "    # Load the last update date\n",
    "    last_update = load_last_update_date(file_path)\n",
    "    if last_update is None:\n",
    "        last_update = datetime.utcnow() - timedelta(days=30)  # Default to last 30 days\n",
    "    else:\n",
    "        last_update = pd.to_datetime(last_update)  # Convert to datetime\n",
    "    \n",
    "    # Get the current date\n",
    "    today = datetime.utcnow()\n",
    "\n",
    "    # Stop execution if last_update and today are the same\n",
    "    if last_update.date() == today.date():\n",
    "        print(\"Data is already up-to-date. No new data to fetch.\")\n",
    "        return\n",
    "    \n",
    "    # Retrieve news from Alpaca\n",
    "    new_news = get_new_news(symbols, last_update, today)\n",
    "    \n",
    "    # Convert new news data to DataFrame\n",
    "    new_news_df = pd.DataFrame(new_news)\n",
    "\n",
    "    # Keep only the necessary columns and prepare the data\n",
    "    if not new_news_df.empty:\n",
    "        new_news_df['created_at'] = pd.to_datetime(new_news_df['created_at'])\n",
    "        new_news_df['date'] = new_news_df['created_at'].dt.date\n",
    "        new_news_df = new_news_df.explode('symbols')\n",
    "        new_news_df['sentiment'], new_news_df['sentiment_score'] = zip(*new_news_df['headline'].apply(analyze_sentiment))\n",
    "        new_news_df = new_news_df[['date', 'symbols', 'sentiment_score']]\n",
    "        daily_sentiment = new_news_df.groupby(['date', 'symbols'])['sentiment_score'].mean().reset_index()\n",
    "        daily_sentiment.columns = ['date', 'symbol', 'average_sentiment_score']\n",
    "    else:\n",
    "        daily_sentiment = pd.DataFrame()\n",
    "\n",
    "    # Fetch historical stock data\n",
    "    historical_data = get_historical_stock_data(symbols, last_update, today)\n",
    "\n",
    "    historical_data['date'] = pd.to_datetime(historical_data['time']).dt.date\n",
    "\n",
    "    # Merge historical stock data with sentiment data\n",
    "    stock_data_merged = pd.merge(historical_data, daily_sentiment, how='left', on=['date', 'symbol'])\n",
    "\n",
    "    # Load existing stock data\n",
    "    try:\n",
    "        existing_data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        existing_data = pd.DataFrame()\n",
    "\n",
    "    # Combine the new merged data with the existing data\n",
    "    combined_data = pd.concat([existing_data, stock_data_merged], ignore_index=True)\n",
    "    \n",
    "    # Save the updated DataFrame\n",
    "    combined_data.to_csv(file_path, index=False)\n",
    "    \n",
    "    # Save the most recent update date\n",
    "    save_last_update_date(today, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_sp500_stocks = [\n",
    "    'AAPL',  # Apple Inc.\n",
    "    'MSFT',  # Microsoft Corporation\n",
    "    'AMZN',  # Amazon.com Inc.\n",
    "    'NVDA',  # NVIDIA Corporation\n",
    "    'GOOGL', # Alphabet Inc. (Class A)\n",
    "    'GOOG',  # Alphabet Inc. (Class C)\n",
    "    'TSLA',  # Tesla Inc.\n",
    "    'META',  # Meta Platforms Inc.\n",
    "    'BRK.B', # Berkshire Hathaway Inc. (Class B)\n",
    "    'UNH',   # UnitedHealth Group Incorporated\n",
    "    'JNJ',   # Johnson & Johnson\n",
    "    'XOM',   # Exxon Mobil Corporation\n",
    "    'V',     # Visa Inc.\n",
    "    'PG',    # Procter & Gamble Co.\n",
    "    'JPM',   # JPMorgan Chase & Co.\n",
    "    'LLY',   # Eli Lilly and Company\n",
    "    'MA',    # Mastercard Incorporated\n",
    "    'HD',    # The Home Depot Inc.\n",
    "    'CVX',   # Chevron Corporation\n",
    "    'MRK',   # Merck & Co. Inc.\n",
    "    'PEP',   # PepsiCo Inc.\n",
    "    'ABBV',  # AbbVie Inc.\n",
    "    'KO',    # The Coca-Cola Company\n",
    "    'PFE',   # Pfizer Inc.\n",
    "    'AVGO',  # Broadcom Inc.\n",
    "    'COST',  # Costco Wholesale Corporation\n",
    "    'MCD',   # McDonald's Corporation\n",
    "    'TMO',   # Thermo Fisher Scientific Inc.\n",
    "    'WMT',   # Walmart Inc.\n",
    "    'DHR',   # Danaher Corporation\n",
    "    'NKE',   # NIKE Inc.\n",
    "    'DIS',   # The Walt Disney Company\n",
    "    'ADBE',  # Adobe Inc.\n",
    "    'NFLX',  # Netflix Inc.\n",
    "    'VZ',    # Verizon Communications Inc.\n",
    "    'CSCO',  # Cisco Systems Inc.\n",
    "    'ABT',   # Abbott Laboratories\n",
    "    'ACN',   # Accenture plc\n",
    "    'NEE',   # NextEra Energy Inc.\n",
    "    'LIN',   # Linde plc\n",
    "    'TXN',   # Texas Instruments Incorporated\n",
    "    'MDT',   # Medtronic plc\n",
    "    'PM',    # Philip Morris International Inc.\n",
    "    'WFC',   # Wells Fargo & Company\n",
    "    'HON',   # Honeywell International Inc.\n",
    "    'QCOM',  # QUALCOMM Incorporated\n",
    "    'BMY',   # Bristol-Myers Squibb Company\n",
    "    'LOW',   # Lowe's Companies Inc.\n",
    "    'UNP',   # Union Pacific Corporation\n",
    "    'RTX'    # Raytheon Technologies Corporation\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is already up-to-date. No new data to fetch.\n"
     ]
    }
   ],
   "source": [
    "update_stock_data(top_50_sp500_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol                       time     open    high     low    close  volume  \\\n",
      "0   AAPL  2024-07-25 00:00:00-04:00  218.880  220.81  214.64  217.420  642703   \n",
      "1   AAPL  2024-07-26 00:00:00-04:00  218.940  219.48  216.04  218.030  661067   \n",
      "2   AAPL  2024-07-29 00:00:00-04:00  217.375  219.28  215.79  218.185  381966   \n",
      "3   AAPL  2024-07-30 00:00:00-04:00  219.300  220.27  216.12  218.680  559407   \n",
      "4   AAPL  2024-07-31 00:00:00-04:00  221.520  223.81  220.91  222.180  549326   \n",
      "\n",
      "         date  average_sentiment_score  log_sentiment_score  \n",
      "0  2024-07-25                 0.947577            -0.053847  \n",
      "1  2024-07-26                 0.972036            -0.028362  \n",
      "2  2024-07-29                 0.955637            -0.045377  \n",
      "3  2024-07-30                 0.964756            -0.035880  \n",
      "4  2024-07-31                 0.989995            -0.010056  \n"
     ]
    }
   ],
   "source": [
    "# read stock data and display head\n",
    "\n",
    "stock_data = pd.read_csv(\"stock_data.csv\")\n",
    "print(stock_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up nan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
