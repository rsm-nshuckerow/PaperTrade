{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "import alpaca_trade_api as tradeapi\n",
    "import pandas as pd\n",
    "from transformers import pipeline, BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpaca API credentials\n",
    "ALPACA_API_KEY = \"PKDFQIPQCXE38TAG1WKG\"\n",
    "ALPACA_SECRET_KEY = \"LRaWwamsMm0WDa58x8S0z8wje9gRcYhcgtMf7C55\"\n",
    "ALPACA_URL = 'https://paper-api.alpaca.markets'\n",
    "\n",
    "# Initialize Alpaca API\n",
    "alpaca = tradeapi.REST(ALPACA_API_KEY, ALPACA_SECRET_KEY, base_url=ALPACA_URL, api_version='v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7753, 0.0675, 0.0395],\n",
      "        [0.7243, 0.4809, 0.8803],\n",
      "        [0.9697, 0.9692, 0.9235],\n",
      "        [0.1724, 0.8568, 0.4185],\n",
      "        [0.6877, 0.2481, 0.5717]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"yiyanghkust/finbert-tone\"  # Example of a financial sentiment model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into 512-token chunks based on tokenization\n",
    "def split_into_chunks(text, max_length=500):\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=False)['input_ids'][0]\n",
    "    # Ensure each chunk is no more than 512 tokens\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunk = tokens[i:i + max_length]\n",
    "        # Make sure the chunk is exactly 512 tokens or less\n",
    "        if len(chunk) > max_length:\n",
    "            chunk = chunk[:max_length]\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze sentiment for long texts\n",
    "def analyze_sentiment(text):\n",
    "    chunks = split_into_chunks(text)\n",
    "    sentiments = []\n",
    "    for chunk in chunks:\n",
    "        # Convert tokens back to text before sentiment analysis\n",
    "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        sentiments.append(nlp(chunk_text)[0])\n",
    "    \n",
    "    # Aggregate sentiment scores (e.g., by averaging)\n",
    "    avg_sentiment_score = sum(s['score'] for s in sentiments) / len(sentiments)\n",
    "    # Determine overall sentiment by majority vote or averaging\n",
    "    positive_scores = sum(s['score'] for s in sentiments if s['label'] == 'positive')\n",
    "    negative_scores = sum(s['score'] for s in sentiments if s['label'] == 'negative')\n",
    "    sentiment_label = 'positive' if positive_scores >= negative_scores else 'negative'\n",
    "    \n",
    "    return sentiment_label, avg_sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_stock_data(symbols, start_date, end_date):\n",
    "    # Use the correct TimeFrame object for daily data\n",
    "    timeframe = tradeapi.TimeFrame.Day\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        bars = alpaca.get_bars(\n",
    "            symbol,\n",
    "            timeframe=timeframe,\n",
    "            start=start_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            end=end_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            adjustment='raw',\n",
    "            feed='iex'\n",
    "        )\n",
    "        \n",
    "        data = []\n",
    "        for bar in bars:\n",
    "            data.append({\n",
    "                'symbol': symbol,  # Add the symbol to the data\n",
    "                'time': bar.t,\n",
    "                'open': bar.o,\n",
    "                'high': bar.h,\n",
    "                'low': bar.l,\n",
    "                'close': bar.c,\n",
    "                'volume': bar.v\n",
    "            })\n",
    "        \n",
    "        all_data.extend(data)\n",
    "    \n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the most recent date from the CSV file\n",
    "def load_last_update_date(file_path=\"stock_data.csv\"):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure that the 'date' column is converted to datetime, coercing errors\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "        # Drop rows where 'date' couldn't be converted to datetime (i.e., NaT)\n",
    "        df = df.dropna(subset=['date'])\n",
    "\n",
    "        # Find the most recent date\n",
    "        most_recent_date = df['date'].max()\n",
    "\n",
    "        return most_recent_date\n",
    "    except (FileNotFoundError, IndexError, KeyError):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the current date as the last update date\n",
    "def save_last_update_date(date, file_path=\"stock_data.csv\"):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.loc[df['date'].idxmax(), 'date'] = date\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get news from the Alpaca API\n",
    "def get_new_news(symbols, start_date, end_date):\n",
    "    url = \"https://data.alpaca.markets/v1beta1/news\"\n",
    "    headers = {\n",
    "        \"APCA-API-KEY-ID\": ALPACA_API_KEY,\n",
    "        \"APCA-API-SECRET-KEY\": ALPACA_SECRET_KEY\n",
    "    }\n",
    "    news = []\n",
    "    page_token = None\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            \"symbols\": \",\".join(symbols),\n",
    "            \"start\": start_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            \"end\": end_date.strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            \"limit\": 50,\n",
    "            \"page_token\": page_token\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            \n",
    "            news.extend(result.get('news', []))\n",
    "            \n",
    "            page_token = result.get('next_page_token')\n",
    "            if not page_token:\n",
    "                break\n",
    "            \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 429:\n",
    "                print(\"Rate limit reached. Sleeping for 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(f\"HTTP error occurred: {e}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "        \n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_stock_data(symbols, file_path=\"stock_data.csv\"):\n",
    "    # Load the last update date\n",
    "    last_update = load_last_update_date(file_path)\n",
    "    if last_update is None:\n",
    "        last_update = datetime.utcnow() - timedelta(days=30)  # Default to last 30 days\n",
    "    else:\n",
    "        last_update = pd.to_datetime(last_update)  # Convert to datetime\n",
    "    \n",
    "    # Get the current date\n",
    "    today = datetime.utcnow()\n",
    "\n",
    "    # Stop execution if last_update and today are the same\n",
    "    if last_update.date() == today.date():\n",
    "        print(\"Data is already up-to-date. No new data to fetch.\")\n",
    "        return\n",
    "    \n",
    "    # Retrieve news from Alpaca\n",
    "    new_news = get_new_news(symbols, last_update, today)\n",
    "    \n",
    "    # Convert new news data to DataFrame\n",
    "    new_news_df = pd.DataFrame(new_news)\n",
    "\n",
    "    # Keep only the necessary columns and prepare the data\n",
    "    if not new_news_df.empty:\n",
    "        new_news_df['created_at'] = pd.to_datetime(new_news_df['created_at'])\n",
    "        new_news_df['date'] = new_news_df['created_at'].dt.date\n",
    "        new_news_df = new_news_df.explode('symbols')\n",
    "        new_news_df['sentiment'], new_news_df['sentiment_score'] = zip(*new_news_df['headline'].apply(analyze_sentiment))\n",
    "        new_news_df = new_news_df[['date', 'symbols', 'sentiment_score']]\n",
    "        daily_sentiment = new_news_df.groupby(['date', 'symbols'])['sentiment_score'].mean().reset_index()\n",
    "        daily_sentiment.columns = ['date', 'symbol', 'average_sentiment_score']\n",
    "    else:\n",
    "        daily_sentiment = pd.DataFrame()\n",
    "\n",
    "    # Fetch historical stock data\n",
    "    historical_data = get_historical_stock_data(symbols, last_update, today)\n",
    "\n",
    "    historical_data['date'] = pd.to_datetime(historical_data['time']).dt.date\n",
    "\n",
    "    # Merge historical stock data with sentiment data\n",
    "    stock_data_merged = pd.merge(historical_data, daily_sentiment, how='left', on=['date', 'symbol'])\n",
    "\n",
    "    # Load existing stock data\n",
    "    try:\n",
    "        existing_data = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        existing_data = pd.DataFrame()\n",
    "\n",
    "    # Combine the new merged data with the existing data\n",
    "    combined_data = pd.concat([existing_data, stock_data_merged], ignore_index=True)\n",
    "    \n",
    "    # Save the updated DataFrame\n",
    "    combined_data.to_csv(file_path, index=False)\n",
    "    \n",
    "    # Save the most recent update date\n",
    "    save_last_update_date(today, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_sp500_stocks = [\n",
    "    'AAPL',  # Apple Inc.\n",
    "    'MSFT',  # Microsoft Corporation\n",
    "    'AMZN',  # Amazon.com Inc.\n",
    "    'NVDA',  # NVIDIA Corporation\n",
    "    'GOOGL', # Alphabet Inc. (Class A)\n",
    "    'GOOG',  # Alphabet Inc. (Class C)\n",
    "    'TSLA',  # Tesla Inc.\n",
    "    'META',  # Meta Platforms Inc.\n",
    "    'BRK.B', # Berkshire Hathaway Inc. (Class B)\n",
    "    'UNH',   # UnitedHealth Group Incorporated\n",
    "    'JNJ',   # Johnson & Johnson\n",
    "    'XOM',   # Exxon Mobil Corporation\n",
    "    'V',     # Visa Inc.\n",
    "    'PG',    # Procter & Gamble Co.\n",
    "    'JPM',   # JPMorgan Chase & Co.\n",
    "    'LLY',   # Eli Lilly and Company\n",
    "    'MA',    # Mastercard Incorporated\n",
    "    'HD',    # The Home Depot Inc.\n",
    "    'CVX',   # Chevron Corporation\n",
    "    'MRK',   # Merck & Co. Inc.\n",
    "    'PEP',   # PepsiCo Inc.\n",
    "    'ABBV',  # AbbVie Inc.\n",
    "    'KO',    # The Coca-Cola Company\n",
    "    'PFE',   # Pfizer Inc.\n",
    "    'AVGO',  # Broadcom Inc.\n",
    "    'COST',  # Costco Wholesale Corporation\n",
    "    'MCD',   # McDonald's Corporation\n",
    "    'TMO',   # Thermo Fisher Scientific Inc.\n",
    "    'WMT',   # Walmart Inc.\n",
    "    'DHR',   # Danaher Corporation\n",
    "    'NKE',   # NIKE Inc.\n",
    "    'DIS',   # The Walt Disney Company\n",
    "    'ADBE',  # Adobe Inc.\n",
    "    'NFLX',  # Netflix Inc.\n",
    "    'VZ',    # Verizon Communications Inc.\n",
    "    'CSCO',  # Cisco Systems Inc.\n",
    "    'ABT',   # Abbott Laboratories\n",
    "    'ACN',   # Accenture plc\n",
    "    'NEE',   # NextEra Energy Inc.\n",
    "    'LIN',   # Linde plc\n",
    "    'TXN',   # Texas Instruments Incorporated\n",
    "    'MDT',   # Medtronic plc\n",
    "    'PM',    # Philip Morris International Inc.\n",
    "    'WFC',   # Wells Fargo & Company\n",
    "    'HON',   # Honeywell International Inc.\n",
    "    'QCOM',  # QUALCOMM Incorporated\n",
    "    'BMY',   # Bristol-Myers Squibb Company\n",
    "    'LOW',   # Lowe's Companies Inc.\n",
    "    'UNP',   # Union Pacific Corporation\n",
    "    'RTX'    # Raytheon Technologies Corporation\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"2024-08-27\" doesn't match format \"%Y-%m-%d %H:%M:%S.%f\", at position 24. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mupdate_stock_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_50_sp500_stocks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 56\u001b[0m, in \u001b[0;36mupdate_stock_data\u001b[0;34m(symbols, file_path)\u001b[0m\n\u001b[1;32m     53\u001b[0m combined_data\u001b[38;5;241m.\u001b[39mto_csv(file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Save the most recent update date\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43msave_last_update_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoday\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36msave_last_update_date\u001b[0;34m(date, file_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_last_update_date\u001b[39m(date, file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstock_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      3\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m----> 4\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     df\u001b[38;5;241m.\u001b[39mloc[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39midxmax(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m date\n\u001b[1;32m      6\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:1063\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1061\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[0;32m-> 1063\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:247\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    245\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[0;32m--> 247\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[1;32m    436\u001b[0m     arg,\n\u001b[1;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:467\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[0;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[1;32m    457\u001b[0m     arg,\n\u001b[1;32m    458\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    462\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m     result, tz_out \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m         unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32mstrptime.pyx:501\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:451\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:583\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data \"2024-08-27\" doesn't match format \"%Y-%m-%d %H:%M:%S.%f\", at position 24. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "update_stock_data(top_50_sp500_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  symbol                       time     open    high     low    close  volume  \\\n",
      "0   AAPL  2024-07-25 00:00:00-04:00  218.880  220.81  214.64  217.420  642703   \n",
      "1   AAPL  2024-07-26 00:00:00-04:00  218.940  219.48  216.04  218.030  661067   \n",
      "2   AAPL  2024-07-29 00:00:00-04:00  217.375  219.28  215.79  218.185  381966   \n",
      "3   AAPL  2024-07-30 00:00:00-04:00  219.300  220.27  216.12  218.680  559407   \n",
      "4   AAPL  2024-07-31 00:00:00-04:00  221.520  223.81  220.91  222.180  549326   \n",
      "\n",
      "                         date  average_sentiment_score  log_sentiment_score  \n",
      "0  2024-07-25 00:00:00.000000                 0.947577            -0.053847  \n",
      "1  2024-07-26 00:00:00.000000                 0.972036            -0.028362  \n",
      "2  2024-07-29 00:00:00.000000                 0.955637            -0.045377  \n",
      "3  2024-07-30 00:00:00.000000                 0.964756            -0.035880  \n",
      "4  2024-07-31 00:00:00.000000                 0.989995            -0.010056  \n"
     ]
    }
   ],
   "source": [
    "# read stock data and display head\n",
    "\n",
    "stock_data = pd.read_csv(\"stock_data.csv\")\n",
    "print(stock_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
